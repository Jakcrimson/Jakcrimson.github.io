---
title: Deep Learning - Denoising Autoencoder
author:
  name: Pierre Lague & Ilian VANDENBERGHE
  link: 
date: 2024-03-04 09:45:00 +0800
categories: [Studies, U-Lille, Python]
tags: [Python, ML]
math: true
mermaid: true
image:
  src: '/assets/posts/umap/header.png'
  width: 800
  height: 600
---

# Deep Learning - Autoencoders üîâ

* [Etude Autoencodeur](#etude-ae)
    * [Exploration et cr√©ation des bruits](#exploration-et-cr√©ation-des-bruits)
    * [Cr√©ation du jeu de donn√©es bruit√©es](#cr√©ation-dun-dataset-bruit√©)
    * [Cr√©ation dataloaders](#cr√©ation-des-dataloaders-√†-partir-des-datasets)
    * [D√©finition de l'Autoencodeur de d√©bruitage (classes 1 et 7)](#definition-de-lautoencodeur-de-d√©bruitage)
        * [D√©finition de l'optimisateur et loss](#definition-de-loptimisateur-de-la-loss-et-du-mod√®le)
        * [M√©thode d'entra√Ænement](#definition-de-la-methode-dentra√Ænement-du-mod√®le)
        * [Premiers r√©sultats du mod√®le](#r√©sultats-de-lentra√Ænement-du-mod√®le)
        * [Optimisation de l'architecture du mod√®le](#optimisation-de-larchitecture-du-mod√®le)
        * [Visualisation des pr√©dictions](#visualisation-des-pr√©dictions-du-mod√®le-sur-des-images-bruit√©es)

    * [Extension du mod√®le sur toutes les classes MNIST](#mod√®le-sur-lensemble-des-donn√©es-mnist)
    * [Impl√©mentation d'architectures d√©riv√©es](#impl√©mentation-de-nouvelles-architectures-d√©riv√©es)
        * [D√©finition du classifieur benchmark CNN](#d√©finition-du-classifier-benchmark-cnn)
            * [R√©sultats Benchmark CNN](#r√©sultat-benchmark-cnn)
        * [D√©finition du classifieur Encoder-MLP](#definition-du-classifier-encoder-mlp)
            * [Cr√©ation du trainset avec les zones latentes](#cr√©ation-du-jeu-dentra√Ænement-avec-les-zones-latentes)
            * [R√©sultats du classifieur Encoder-MLP](#r√©sultats-du-classifieur-encoder-mlp)
            * [GridSearch mod√®le Encodeur-MLP](#gridsearch-mod√®le-encodeur-mlp)
    * [Discussion sur les r√©sultats obtenus](#discussion-sur-les-performances-de-mod√®les-encodeur-cnn-et-encodeur-mlp)



# Autoencodeurs <a id="etude-ae"></a>

Un autoencodeur est un r√©seau de neurones utilis√© pour la r√©duction de dimension ; c'est-√†-dire pour la s√©lection et l'extraction des caract√©ristiques. Les autoencodeurs avec plus de couches cach√©es que d'entr√©es courent le risque d'apprendre la fonction identit√© - o√π la sortie est simplement √©gale √† l'entr√©e - devenant ainsi inutiles.
Les autoencodeurs de d√©bruitage sont une extension de l'autoencodeur de base, et repr√©sentent une version stochastique de celui-ci. Les autoencodeurs de d√©bruitage tentent de r√©soudre le risque de la fonction identit√© en corrompant al√©atoirement l'entr√©e (c'est-√†-dire en introduisant du bruit) que l'autoencodeur doit ensuite reconstruire, ou d√©bruiter.

Une premi√®re partie de l'√©tude se concentre sur ces autoendcodeur de d√©bruitage, l'optimisation de leur architecture et hyperparam√®tres ainsi que leur capacit√© √† √™tre robuste quand ils sont expos√©s √† de nouvelles donn√©es.  Cette premi√®re partie se concentre √©galmement sur l'importance de l'apprentissage de repr√©sentation et les caract√©ristiques de la zone latente.


```python
import json
import torch
import numpy as np
from tqdm import tqdm
import torch.nn as nn
from typing import Callable
import torch.optim as optim
import matplotlib.pyplot as plt
import torch.nn.functional as F
from keras.datasets import mnist
from torchvision import transforms
from sklearn.preprocessing import minmax_scale
from sklearn.metrics import mean_squared_error
from torch.utils.data import DataLoader,Dataset
from sklearn.model_selection import ParameterGrid

#We check whether cuda is available and choose device accordingly
if torch.cuda.is_available() == True:
  device = "cuda:0"
else:
  device = "cpu"
```

### Exploration et cr√©ation des bruits <a id="exploration-et-cr√©ation-des-bruits"></a>

>Justification des choix des donn√©es :

Dans le but de mieux appr√©hender le fonctionnement des autoencodeurs et d'illustrer leur capacit√© g√©n√©rative, nous avons pris la d√©cision de construire un ensemble de donn√©es exclusivement compos√© d'images repr√©sentant les chiffres 1 et 7 (MNIST Digits). Les chiffres 1 et 7 pr√©sentent des similitudes marqu√©es dans leur √©criture. En introduisant divers types de bruits (dont nous discuterons ult√©rieurement), il devient possible de constituer un ensemble de donn√©es tr√®s vari√© et d'entra√Æner un mod√®le robuste. Par souci de performance et de faisabilit√© logistique, nous avons restreint notre choix √† deux chiffres (1 et 7). Cependant, une exploration ult√©rieure sera propos√©e, o√π nous inclurons tous les chiffres du dataset MNIST.

>Justification des choix des bruits :

Deux types de bruits sont d√©j√† impl√©ment√©s, √† savoir le "speckle" et le "gaussian".
- Le bruit speckle, √©galement appel√© bruit de chatoiement, fait r√©f√©rence √† toute fluctuation parasite ou toute d√©gradation subie par une image depuis le moment de son acquisition jusqu'√† son enregistrement. Ce bruit illustre un cas concret o√π l'image est captur√©e par un appareil num√©rique puis transmise au mod√®le, la pr√©sence de bruit √©tant in√©vitable. Ce type de bruit se distingue par sa capacit√© √† s√©parer le premier plan de l'arri√®re-plan, ce qui met en valeur le chiffre tout en le d√©gradant.
- Le bruit gaussien, en traitement du signal, est un bruit caract√©ris√© par une densit√© de probabilit√© qui suit une distribution gaussienne (loi normale). Les principales sources de bruit gaussien dans les images num√©riques surviennent pendant l'acquisition, par exemple le bruit du capteur caus√© par un mauvais √©clairage et/ou une temp√©rature √©lev√©e. Ce bruit uniforme permet au mod√®le d'√™tre plus r√©silient dans sa classification car il n'accentue pas le chiffre mais uniformise la d√©gradation de l'image.
- Le bruit "salt and pepper", ou bruit sel ou poivre, correspond √† une alt√©ration al√©atoire subie par une image num√©rique, entra√Ænant la modification de l'intensit√© de certains pixels (r√©partis de mani√®re al√©atoire dans l'image) jusqu'√† atteindre la valeur minimale ou maximale de la plage dynamique du pixel, respectivement 0 et 255 dans le cas d'une image num√©rique cod√©e en 8 bits. Ce type de bruit permet au mod√®le de traiter des valeurs extr√™mes (0 ou 255).

Chacun des bruits sera scal√© avec un param√®tre `scale` qui permet de modifier l'intensit√© de la pr√©sence du bruit dans l'image. En jouant sur les valeurs de ce param√®tre nous pourront cr√©er des donn√©es plus ou moins bruit√©es et donc tester les limites de robustesse de nos mod√®les ainsi que de leur capacit√© √† apprendre m√™me avec beaucoup de bruit.




```python
"""
Here we load the dataset, add gaussian,poisson,speckle

    'gauss'     Gaussian-distributed additive noise.
    'speckle'   Multiplicative noise using out = image + n*image,where
                n is uniform noise with specified mean & variance.
    'salt_and_pepper' Randomly chosen pixels that are converted in black or white based on a probability.
    'scale' : the scale at which the noise is present in the image. It allows to create images with little to no noise and images with a lot of noise (for model robustness).
                
We define a function that adds each noise when called from main function
Input & Output: np array
"""

def add_noise(img, noise_type="gaussian", scale=0.5):
  row, col = 28, 28
  img = img.astype(np.float32)

  match noise_type:
    case "gaussian":
      mean  = 0
      var   = 0.01
      sigma = var**.5
      noise = np.random.normal(mean, sigma, img.shape)
      noise = noise.reshape(row, col)
      img   = img + (noise*scale)
      return img
     
    case "speckle" :
      noise = np.random.randn(row, col)
      noise = noise.reshape(row, col)
      img   = img + (img*noise*scale)
      return img
     
    case "salt_pepper" :
      prob = 0.4*scale # 1/4 is the base probability of a pixel being transformed. It is scaled down or up with the scale parameter
      output = img.copy()
      if len(img.shape) == 2:
          black = 0
          white = 1
        
      probs = np.random.random(output.shape[:2])
      output[probs < (prob/2)] = black
      output[probs > 1-(prob/2)] = white
      return output     
    case _:
      return img

```


```python
mnist_ = mnist.load_data()

# selection des chiffres 1 et 7 pour constituer nos jeux de donn√©es
train_mask = (mnist_[0][1] == 1) | (mnist_[0][1] == 7)
test_mask = (mnist_[1][1] == 1) | (mnist_[1][1] == 7)

(xtrain, ytrain) = mnist_[0][0][train_mask], mnist_[0][1][train_mask]
(xtest, ytest) = mnist_[1][0][test_mask], mnist_[1][1][test_mask]
```


```python
xtrain = np.array([minmax_scale(x) for x in xtrain])
xtest = np.array([minmax_scale(x) for x in xtest])
```


```python
print("No of training datapoints:{}\nNo of Test datapoints:{}".format(len(xtrain), len(xtest)))
```

    No of training datapoints:13007
    No of Test datapoints:2163
    


```python
"""
From here onwards,we split the 60k training datapoints into 3 sets each given one type of each noise.
We shuffle them for better generalization.
"""
noises = ["gaussian", "speckle", "salt_pepper"]
noise_ct = 0
noise_id = 0
traindata = np.zeros((len(xtrain), 28, 28))


for idx in tqdm(range(len(xtrain))):
  if noise_ct < len(xtrain)/3:
    noise_ct += 1
    traindata[idx] = add_noise(xtrain[idx], noise_type=noises[noise_id], scale=1)
  else:
    print("\n{} noise addition completed to images".format(noises[noise_id]))
    noise_id += 1
    noise_ct = 0


print("\n{} noise addition completed to images".format(noises[noise_id])) 


noise_ct = 0
noise_id = 0
testdata = np.zeros((10000, 28, 28))

for idx in tqdm(range(len(xtest))):
  if noise_ct < len(xtest)/3:
    noise_ct += 1
    x = add_noise(xtest[idx], noise_type=noises[noise_id])
    testdata[idx] = x
    
  else:
    print("\n{} noise addition completed to images".format(noises[noise_id]))
    noise_id += 1
    noise_ct = 0

print("\n{} noise addition completed to images".format(noises[noise_id]))    
  
```

     24%|‚ñà‚ñà‚ñç       | 3111/13007 [00:00<00:00, 31104.39it/s]

    
    gaussian noise addition completed to images
    

     78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 10145/13007 [00:00<00:00, 15374.61it/s]

    
    speckle noise addition completed to images
    

    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13007/13007 [00:01<00:00, 12024.46it/s]
    

    
    salt_pepper noise addition completed to images
    

    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2163/2163 [00:00<00:00, 12682.08it/s]

    
    gaussian noise addition completed to images
    
    speckle noise addition completed to images
    
    salt_pepper noise addition completed to images
    

    
    


```python
"""
Here we Try to visualize, each type of noise that was introduced in the images
Along with their original versions

"""

f, axes=plt.subplots(2,3)

#showing images with gaussian noise
axes[0,0].imshow(xtrain[0], cmap="gray")
axes[0,0].set_title("Original Image")
axes[1,0].imshow(traindata[0], cmap='gray')
axes[1,0].set_title("Gaussian Noised Image")

#showing images with salt and pepper noise
axes[0,1].imshow(xtrain[13000], cmap='gray')
axes[0,1].set_title("Original Image")
axes[1,1].imshow(traindata[13000], cmap="gray")
axes[1,1].set_title("S&P Noised Image")

#showing imags with the speckle noise
axes[0,2].imshow(xtrain[5000], cmap='gray')
axes[0,2].set_title("Original Image")
axes[1,2].imshow(traindata[5000], cmap="gray")
axes[1,2].set_title("Speckle Noised Image")

plt.tight_layout()
```


    
![png](../assets/posts/autoencodeur/DenoisingAutoencoder/output_11_0.png)
    


### Cr√©ation d'un dataset bruit√© <a class="anchor" id="cr√©ation-dun-dataset-bruit√©"></a>

La classe NoisedDataset, comme son nom l'indique est un dataset compos√© d'images bruit√©es. Le dataset est compos√© de tuples ayant la structure suivante :

$x = (image\_bruit√©e, image\_originale, label)$

Les types de bruits sont r√©partis uniform√©ment dans le dataset de sorte √† ce qu'il n'y ait pas de biais/d√©s√©quilibre.
Les fonctions de classe __len__ et __getitem__ vont nous permettre d'utiliser cette classe pour cr√©er des dataloader rendant ainsi plus rapide le passage d'images en batch pendant l'entra√Ænement.


```python
class noisedDataset(Dataset):
  def __init__(self, 
               datasetnoised: np.ndarray, 
               datasetclean: np.ndarray, 
               labels: np.ndarray,
               transform: Callable):
    self.noise = datasetnoised
    self.clean = datasetclean
    self.labels = labels
    self.transform = transform
  
  def __len__(self):
    return len(self.noise)
  
  def __getitem__(self, idx):
    xNoise = self.noise[idx]
    xClean = self.clean[idx]
    y = self.labels[idx]
    
    if self.transform != None:
      xNoise = self.transform(xNoise)
      xClean = self.transform(xClean)

    return (xNoise, xClean, y)
    
```


```python
tsfms = transforms.Compose([
    transforms.ToTensor()
])

trainset = noisedDataset(traindata, xtrain, ytrain, tsfms)
testset = noisedDataset(testdata, xtest, ytest, tsfms)
```

#### Cr√©ation des dataloaders √† partir des datasets <a class="anchor" id="cr√©ation-des-dataloaders-√†-partir-des-datasets"></a>


```python
"""
Here , we create the trainloaders and testloaders.
Also, we transform the images using standard lib functions
"""

trainloader = DataLoader(trainset, batch_size=32, shuffle=True)
testloader  = DataLoader(testset, batch_size=1, shuffle=True)
```

### Definition de l'Autoencodeur de d√©bruitage <a class="anchor" id="definition-de-lautoencodeur-de-d√©bruitage"></a>

Cet autoencodeur de d√©bruitage est capable de prendre en entr√©e des images (ou des vecteurs) et de les encoder dans un espace de dimension r√©duite appel√© "zone latente". Il les d√©code ensuite √† partir de cet espace latent pour reconstruire l'entr√©e initiale sans le bruit.

Voici une explication des principaux √©l√©ments :

- La classe *denoising_model* est une sous-classe de nn.Module, ce qui signifie qu'elle h√©rite des fonctionnalit√©s de base de PyTorch pour les mod√®les de r√©seaux de neurones.

- L'initialisateur __init__ de la classe permet de d√©finir la structure de l'autoencodeur en sp√©cifiant le nombre de couches cach√©es (hidden_layer_nb), le nombre de neurones dans chaque couche cach√©e (hidden_size), le nombre de neurones dans la zone latente (latent_size) et la taille de l'image en entr√©e (img_size).

- L'encodeur est une s√©quence de couches lin√©aires (fully connected) suivies de fonctions d'activation ReLU. Ces couches transforment l'entr√©e en une repr√©sentation de dimension r√©duite dans l'espace latent.

- Le d√©codeur est √©galement une s√©quence de couches lin√©aires suivies de fonctions d'activation ReLU, mais cette fois-ci il restaure l'entr√©e originale √† partir de la repr√©sentation de l'espace latent.

- La fonction forward est une m√©thode requise par PyTorch pour d√©finir comment les donn√©es traversent le r√©seau. Dans ce cas, elle encode d'abord les donn√©es avec l'encodeur puis les d√©code avec le d√©codeur.

- La m√©thode get_latent_size retourne la taille de la zone latente.




```python
"""
Here, we define the autoencoder model.
"""

class denoising_model(nn.Module):
  """
  Autoencodeur, capable d'√™tre personnalis√© au niveau du nombre de neurones dans la / les couche(s) cach√©e(s) et au nombre de couches cach√©es.

  Param√®tres :
  ------------
  `hidden_layer_nb`: int (default=1)  
    Nombre de couche cach√©es
  `hidden_size`: int (default=256)
    Nombre de neurones dans chaque couche de la couche cach√©e
  `latent_size`: int (default=64)  
    Nombre de neurone dans la zone latente
  """
  def __init__(self, hidden_layer_nb=1, hidden_size = 256, latent_size=64, img_size=28*28):
    super(denoising_model,self).__init__()

    self.latent_size = latent_size

    encode_sequence_input = [nn.Linear(img_size, hidden_size), nn.ReLU()]
    encode_sequence_output = [nn.Linear(hidden_size, latent_size), nn.ReLU()]
    encode_sequence_hidden = []
    for i in range(hidden_layer_nb):
      encode_sequence_hidden.append(nn.Linear(hidden_size, hidden_size))
      encode_sequence_hidden.append(nn.ReLU())

    encode_sequence = encode_sequence_input + encode_sequence_hidden + encode_sequence_output
        
    self.encoder = nn.Sequential(
      *encode_sequence
    )

    decode_sequence_input = [nn.Linear(latent_size, hidden_size), nn.ReLU()]
    decode_sequence_output = [nn.Linear(hidden_size, img_size), nn.Sigmoid()]
    decode_sequence_hidden = []
    for i in range(hidden_layer_nb):
      decode_sequence_hidden.append(nn.Linear(hidden_size, hidden_size))
      decode_sequence_hidden.append(nn.ReLU())

    decode_sequence = decode_sequence_input + decode_sequence_hidden + decode_sequence_output

    self.decoder = nn.Sequential(
      *decode_sequence
    )

  def get_latent_size(self):
    return self.latent_size

  def encode(self, x):
    return self.encoder(x)
 
  def forward(self, x):
    x = self.encoder(x)
    x = self.decoder(x)
    
    return x
```

#### Definition de l'optimisateur, de la loss et du mod√®le <a class="anchor" id="definition-de-loptimisateur-de-la-loss-et-du-mod√®le"></a>

Le choix d'une loss adapt√©es est tr√®s importante dans le process d'entra√Ænement d'un mod√®le. La perte de Mean Squared Error (MSE) est appropri√©e pour les autoencodeurs en raison de sa compatibilit√© avec les espaces de sortie continus, de sa sensibilit√© aux erreurs importantes et de sa facilit√© de normalisation avec le calcul de la moyenne. Elle p√©nalise plus lourdement les erreurs importantes.


```python
model = denoising_model(hidden_layer_nb=2).to(device) 
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)
```


```python
print(model)
```

    denoising_model(
      (encoder): Sequential(
        (0): Linear(in_features=784, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=256, bias=True)
        (5): ReLU()
        (6): Linear(in_features=256, out_features=64, bias=True)
        (7): ReLU()
      )
      (decoder): Sequential(
        (0): Linear(in_features=64, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=256, bias=True)
        (5): ReLU()
        (6): Linear(in_features=256, out_features=784, bias=True)
        (7): Sigmoid()
      )
    )
    

### Definition de la methode d'entra√Ænement du mod√®le <a class="anchor" id="definition-de-la-methode-dentra√Ænement-du-mod√®le"></a>

La m√©thode train_test permet de r√©aliser l'entra√Ænement et l'√©valuation d'un mod√®le (`model`) pass√© en param√®tre.

Cette m√©thode permet de sp√©cifier l'optimisateur utilis√© ainsi que la loss, le nombre d'epochs, le device (pour la gpu acceleration), le trainloader et le testloader.
Il nous semblait important de pouvoir sp√©cifier tous ces param√®tres afin de rendre la m√©thode la plus r√©utillisable possible.

Pendant la phase d'entra√Ænement, nous calculons la perte moyenne de l'epoch afin de garder une trace de la performance du mod√®le.


Cette fonction permet d'entra√Ænet le mod√®le mais aussi de l'√©valuer. En effet, apr√®s la phase d'entra√Ænement sur le trainloader, nous calculons la loss totale (MSE) sur le testloader.
Ce calcul est fait manuellement : 

- nous comparons la valeur du label √† la valeur pr√©dite par le mod√®le. En fonction de la nature du testloader, il est important de modifier le type des donn√©es de test afin qu'elles soient compatibles en entr√©e avec le mod√®le (d'o√π les transtypages en tenseurs).
- la true_error est incr√©ment√©e de 1 si le label et la pr√©diction correspondent, 0 sinon

Au final nous retournons l'ensemble des loss au fil des epochs et la true_error.




```python
def train_test(
        model: torch.nn.Module, 
        optimizer: torch.optim.Optimizer, 
        criterion: torch.nn.Module, 
        epochs: int,
        device: str,
        trainloader: torch.utils.data.DataLoader, 
        testset: torch.utils.data.Dataset):
    """
    Fonction pour entra√Æner / tester un mod√®le.

    Parameters:
    -----------
    model : torch.nn.Module
        Le mod√®le √† entra√Æner.
    optimizer : torch.optim.Optimizer
        L'optimiseur utilis√© pour la mise √† jour des poids du mod√®le.
    criterion : torch.nn.Module
        La fonction de perte utilis√©e pour √©valuer la diff√©rence entre les pr√©dictions et les vraies √©tiquettes.
    epochs : int
        Le nombre d'√©poques d'entra√Ænement.
    device : str
        L'appareil sur lequel ex√©cuter l'entra√Ænement, par exemple 'cuda' pour GPU ou 'cpu' pour CPU.
    trainloader : torch.utils.data.DataLoader
        Le DataLoader contenant les donn√©es d'entra√Ænement.
    testset : torch.utils.data.Dataset
        Le jeu de donn√©es de test utilis√© pour √©valuer le mod√®le apr√®s chaque √©poque.

    Returns:
    --------
    true_error : float
        L'erreur moyenne quadratique sur le jeu de donn√©es de test.
    losslist : list
        Liste des valeurs de la fonction de perte moyenne sur les donn√©es d'entra√Ænement pour chaque √©poque.
    """
    model.train()
    losslist = []
    for epoch in tqdm(range(epochs)):
        running_loss = 0
        for dirty, clean, label in trainloader:  
            
            dirty = dirty.view(dirty.size(0), -1).type(torch.FloatTensor)
            clean = clean.view(clean.size(0), -1).type(torch.FloatTensor)
            dirty, clean = dirty.to(device), clean.to(device)

            optimizer.zero_grad()

            # Forward pass
            output = model.forward(dirty)
            loss = criterion(output, clean)

            # R√©tropropagation
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()

        # Calcul de la perte moyenne pour cette √©poque
        losslist.append(running_loss/len(trainloader))

    # Calcul de l'erreur moyenne quadratique sur le jeu de donn√©es de test
    true_error = 0
    for test in testset:
        true_error += mean_squared_error(test.flatten(), model(torch.tensor(test.flatten()).type(torch.FloatTensor)).detach())
    true_error /= len(testset)

    return true_error, losslist
```

Nous entra√Ænons un mod√®le type autoencodeur de b√©ruitage qui nous servira de baseline pour comparer les autres mod√®les.


```python
train_test(model, optimizer, criterion, 120, device, trainloader, testdata)
```

    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 120/120 [16:04<00:00,  8.04s/it]



#### R√©sultats de l'entra√Ænement du mod√®le <a class="anchor" id="r√©sultats-de-lentra√Ænement-du-mod√®le"></a>

Au terme de l'entra√Ænement qui √† dur√© 16 minutes environ, on observe une loss qui d√©croit de 2% √† 1%, ce qui montre une performance relativement bonne du mod√®le pendant son entra√Ænement. Ce score est en accord avec son erreur de generalisation d'environ 1% montrant sa capacit√© √† d√©bruiter des donn√©es non pr√©sentes lors de l'entra√Ænement. A pr√©sent, tentons d'optimiser le mod√®le afin de gagner en performance.

### Optimisation de l'architecture du mod√®le <a class="anchor" id="optimisation-de-larchitecture-du-mod√®le"></a>



```python
def gridsearch(model_obj: nn.Module, 
               parameters: dict,
               optimizer: optim.Optimizer,
               criterion,
               learning_rate :int,
               epoch: int,
               device: str,
               trainloader: DataLoader,
               testdata: Dataset):
    """
    Effectue une recherche sur grille pour trouver les meilleurs hyperparam√®tres pour un mod√®le donn√©.

    Parameters:
    -----------
    model_obj : nn.Module
        Classe du mod√®le √† utiliser pour la recherche sur grille.
    parameters : dict
        Dictionnaire contenant les hyperparam√®tres √† tester.
    optimizer : torch.optim.Optimizer
        L'optimiseur √† utiliser pour la mise √† jour des poids du mod√®le.
    learning_rate : int
        Taux d'apprentissage √† utiliser avec l'optimiseur.
    epoch : int
        Nombre d'√©poques d'entra√Ænement pour chaque configuration de param√®tres.
    device : str
        L'appareil sur lequel ex√©cuter l'entra√Ænement, par exemple 'cuda' pour GPU ou 'cpu' pour CPU.
    trainloader : torch.utils.data.DataLoader
        DataLoader contenant les donn√©es d'entra√Ænement.
    testdata : torch.utils.data.Dataset
        Jeu de donn√©es de test utilis√© pour √©valuer le mod√®le apr√®s chaque configuration de param√®tres.

    Returns:
    --------
    results : dict
        Dictionnaire contenant les r√©sultats de la recherche sur grille.
        Chaque cl√© correspond √† l'indice de la configuration de param√®tres.
        Chaque valeur est un dictionnaire contenant les param√®tres test√©s, l'erreur r√©elle (sur les donn√©es de test)
        et l'erreur d'entra√Ænement (sur les donn√©es d'entra√Ænement) pour cette configuration.
    """
    parameters = ParameterGrid(parameters)
    results = {}
    loss = criterion()

    for i, params in enumerate(parameters):
        tmp_model = model_obj(**params)
        tmp_optim = optimizer(tmp_model.parameters(), lr=learning_rate)
        tmp_result = train_test(tmp_model, tmp_optim, loss, epoch, device, trainloader, testdata)
        print(f"\tParams {i}/{len(parameters)} -> true error: {tmp_result[0]}")
        results[i] = {
            "params": params,
            "true_error": tmp_result[0],
            "training_error": tmp_result[1]
        }

    return results
```


```python
parameters = {"hidden_layer_nb": range(1, 6, 2),
              "hidden_size": range(300, 501, 100),
              "latent_size": [7*7, 8*8, 9*9]}
```


```python
results = gridsearch(denoising_model, parameters, optim.Adam, nn.MSELoss, 1e-4, 30, device, trainloader, testdata)
```
```python
# # on enregistre les r√©sultats pour ne pas devoir refaire la gridsearch
# with open("output_gridsearch.json", "w+") as file:
#     file.write(json.dumps(results, indent=1))

# on lit les r√©sultats de l'ancienne gridsearch
with open("output_gridsearch.json") as file:
    results = json.loads(file.read())
```


```python
[results[str(i)]["params"] for i in range(27) if results[str(i)]["true_error"] == min([results[str(j)]["true_error"] for j in range(27)])][0]
```




    {'hidden_layer_nb': 1, 'hidden_size': 500, 'latent_size': 81}



Afin de trouver le meilleur mod√®le pour cette t√¢che, nous avons r√©alis√© un grid search sur plusieurs param√®tres :
- Le nombre de couches de la couche cach√©e (par le param√®tres `hidden_layer_nb`)
- Le nombre de neurones de la couche cach√©es (par le param√®tres `hidden_size`)
- Le nombre de neurones de la zone latente (par le param√®tres `latent_size`)  

Nous avons opt√© pour des espaces latents sous forme de carr√©s afin de pouvoir les afficher ult√©rieurement dans le m√™me format que les images d'origine, qui sont √©galement carr√©es. Cette approche sera √©galement avantageuse pour la classification, car elle permettra l'utilisation de r√©seaux de convolution tout en pr√©servant les dimensions de base des images.

En raison de contraintes de temps, nous avons limit√© √† seulement 30 it√©rations par test lors de la recherche par grille. Le programme a pris plus d'une heure pour s'ex√©cuter avec seulement ces 30 it√©rations. Nous avons utilis√© le m√™me optimiseur et le m√™me taux d'apprentissage que lors de notre premier test.

Le meilleur mod√®le poss√®de donc cette forme :
```
denoising_model(
  (encoder): Sequential(
    (0): Linear(in_features=784, out_features=500, bias=True)
    (1): ReLU()
    (2): Linear(in_features=500, out_features=500, bias=True)
    (3): ReLU()
    (4): Linear(in_features=500, out_features=81, bias=True)
    (5): ReLU()
  )
  (decoder): Sequential(
    (0): Linear(in_features=81, out_features=500, bias=True)
    (1): ReLU()
    (2): Linear(in_features=500, out_features=500, bias=True)
    (3): ReLU()
    (4): Linear(in_features=500, out_features=784, bias=True)
    (5): Sigmoid()
  )
)
```
C'est √† dire, par rapport aux param√®tres donn√©s :
- Le param√®tre `hidden_layer_nb` vaut 1
- Le param√®tre `hidden_size` vaut 500
- Le param√®tre `latent_size` vaut 81 (donc $9 \times 9$)


```python
best_model = denoising_model(**[results[str(i)]["params"] for i in range(27) if results[str(i)]["true_error"] == min([results[str(j)]["true_error"] for j in range(27)])][0])
best_model_optim = optim.Adam(best_model.parameters(), lr=1e-3)
best_model_criterion = nn.MSELoss()
print(best_model)
```

    denoising_model(
      (encoder): Sequential(
        (0): Linear(in_features=784, out_features=500, bias=True)
        (1): ReLU()
        (2): Linear(in_features=500, out_features=500, bias=True)
        (3): ReLU()
        (4): Linear(in_features=500, out_features=81, bias=True)
        (5): ReLU()
      )
      (decoder): Sequential(
        (0): Linear(in_features=81, out_features=500, bias=True)
        (1): ReLU()
        (2): Linear(in_features=500, out_features=500, bias=True)
        (3): ReLU()
        (4): Linear(in_features=500, out_features=784, bias=True)
        (5): Sigmoid()
      )
    )
    


```python
true_error, losslist = train_test(best_model, best_model_optim, best_model_criterion, 100, device, trainloader, testdata)
# best_model.load_state_dict(torch.load("model", map_location=device))
```

    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [08:13<00:00,  4.94s/it]
    


```python
plt.plot(range(len(losslist)), losslist), true_error
```




    ([<matplotlib.lines.Line2D at 0x734bdfca6b00>], 0.05422814199700569)




    
![png](../assets/posts/autoencodeur/DenoisingAutoencoder/output_36_1.png)
    


### Visualisation des pr√©dictions du mod√®le sur des images bruit√©es  <a class="anchor" id="visualisation-des-pr√©dictions-du-mod√®le-sur-des-images-bruit√©es"></a>

On teste le mod√®le avec des donn√©es bruit√©es. L'intensit√© du bruit est similaire √† celle pr√©sente lors de l'entra√Ænement.


```python
f,axes= plt.subplots(4,3,figsize=(8, 8))
axes[0,0].set_title("Original Image")
axes[0,1].set_title("Dirty Image")
axes[0,2].set_title("Cleaned Image")
for i, noise in enumerate(["gaussian", "speckle", "salt_pepper", ""]):
    dirty = add_noise(np.array(xtest[3]), noise)
    predicted = best_model(torch.tensor(dirty.flatten()).type(torch.FloatTensor))

    axes[i, 0].imshow(xtest[3])
    axes[i, 1].imshow(dirty.reshape((28, 28)))
    axes[i, 2].imshow(predicted.detach().numpy().reshape((28, 28)))
```


    
![png](../assets/posts/autoencodeur/DenoisingAutoencoder/output_39_0.png)
    


L'affichage de ces images montre que notre mod√®le `best_model` est capable de reconstituer l'image originale malgr√®s la pr√©sence de bruit. On remarque m√™me que l'image reconstitu√©e √† des contours plus pr√©cis que l'image originale.

Apr√®s avois √©tabli cette baseline, nous pouvons nous int√©resser √† la robustesse de notre mod√®les. Ceci peut se faire en augmentant le bruit (param√®tre `scale`). Le mod√®le va maintenant avoir des images beaucoup plus bruit√©es que celles sur lesquelles il s'est entra√Æn√©.


```python
f,axes= plt.subplots(4,3,figsize=(8, 8))
axes[0,0].set_title("Original Image")
axes[0,1].set_title("Dirty Image")
axes[0,2].set_title("Cleaned Image")
for i, noise in enumerate(["gaussian", "speckle", "salt_pepper", ""]):
    dirty = add_noise(np.array(xtest[0]), noise, 0.8)
    predicted = best_model(torch.tensor(dirty.flatten()).type(torch.FloatTensor))

    axes[i, 0].imshow(xtest[0])
    axes[i, 1].imshow(dirty.reshape((28, 28)))
    axes[i, 2].imshow(predicted.detach().numpy().reshape((28, 28)))
```


    
![png](../assets/posts/autoencodeur/DenoisingAutoencoder/output_42_0.png)
    



```python
f,axes= plt.subplots(4,3,figsize=(8, 8))
axes[0,0].set_title("Original Image")
axes[0,1].set_title("Dirty Image")
axes[0,2].set_title("Cleaned Image")
for i, noise in enumerate(["gaussian", "speckle", "salt_pepper", ""]):
    dirty = add_noise(np.array(xtest[1]), noise, 1)
    predicted = best_model(torch.tensor(dirty.flatten()).type(torch.FloatTensor))

    axes[i, 0].imshow(xtest[1])
    axes[i, 1].imshow(dirty.reshape((28, 28)))
    axes[i, 2].imshow(predicted.detach().numpy().reshape((28, 28)))
```


    
![png](../assets/posts/autoencodeur/DenoisingAutoencoder/output_43_0.png)
    


Pour le 1 comme pour le 7, ces images montrent que notre mod√®le est capable de reconstituer des images m√™me si le degr√© de bruit est √©lev√©. 
Il est g√©n√©ralisable √† des donn√©es qui n'ont pas le m√™me degr√© de bruit dans les images utilis√©es pour entra√Æner notre mod√®le.


```python
# PATH = "model"
# torch.save(best_model.state_dict(), PATH)  # We save the model state dict at PATH   
```

### Mod√®le sur l'ensemble des donn√©es MNIST <a class="anchor" id="mod√®le-sur-lensemble-des-donn√©es-mnist"></a>

A pr√©sent, nous allons d√©ployer un mod√®le sur l'ensemble des donn√©es MNIST afin de tester la capacit√© qu'√† notre mod√®le √† classifier plusieurs classes.


```python
(xtrain_all, ytrain_all) = mnist_[0][0], mnist_[0][1]
(xtest_all, ytest_all) = mnist_[1][0], mnist_[1][1]

xtrain_all = np.array([minmax_scale(x) for x in xtrain_all])
xtest_all = np.array([minmax_scale(x) for x in xtest_all])
```


```python
"""
From here onwards,we split the 60k training datapoints into 3 sets each given one type of each noise.
We shuffle them for better generalization.
"""
noises = ["gaussian", "speckle", "salt_pepper"]
noise_ct = 0
noise_id = 0
traindata_all = np.zeros((len(xtrain_all), 28, 28))


for idx in tqdm(range(len(xtrain_all))):
  if noise_ct < len(xtrain_all)/3:
    noise_ct += 1
    traindata_all[idx] = add_noise(xtrain_all[idx], noise_type=noises[noise_id], scale=1)
  else:
    print("\n{} noise addition completed to images".format(noises[noise_id]))
    noise_id += 1
    noise_ct = 0


print("\n{} noise addition completed to images".format(noises[noise_id])) 


noise_ct = 0
noise_id = 0
testdata_all = np.zeros((10000, 28, 28))

for idx in tqdm(range(len(xtest_all))):
  if noise_ct < len(xtest)/3:
    x = add_noise(xtest_all[idx], noise_type=noises[noise_id])
    testdata_all[idx] = x
    
  else:
    print("\n{} noise addition completed to images".format(noises[noise_id]))
    noise_id += 1
    noise_ct = 0

print("\n{} noise addition completed to images".format(noises[noise_id]))    
  
```

     45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 27189/60000 [00:00<00:00, 35864.75it/s]

    
    gaussian noise addition completed to images
    

     79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 47623/60000 [00:01<00:00, 42422.04it/s]

    
    speckle noise addition completed to images
    

    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60000/60000 [00:01<00:00, 38322.87it/s]
    

    
    salt_pepper noise addition completed to images
    

    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 40644.92it/s]

    
    gaussian noise addition completed to images
    

    
    


```python
trainset_all = noisedDataset(traindata_all, xtrain_all, ytrain_all, tsfms)
testset_all= noisedDataset(testdata_all, xtest_all, ytest_all, tsfms)
```


```python
trainloader_all = DataLoader(trainset_all, batch_size=500, shuffle=True)
testloader_all  = DataLoader(testset_all, batch_size=1, shuffle=True)
```

Nous reprenons la m√™me architecture que nous avons trouv√© pr√©c√©demment afin d'observer sa capacit√© de g√©n√©ralisation.


```python
best_model_all = denoising_model(**[results[str(j)]["params"] for j in range(27) if results[str(j)]["true_error"] == min([results[str(i)]["true_error"] for i in range(27)])][0])
best_model_criterion_all = nn.MSELoss()
best_model_optim_all = optim.Adam(best_model_all.parameters(), lr=4e-3)
best_model_all
```




    denoising_model(
      (encoder): Sequential(
        (0): Linear(in_features=784, out_features=500, bias=True)
        (1): ReLU()
        (2): Linear(in_features=500, out_features=500, bias=True)
        (3): ReLU()
        (4): Linear(in_features=500, out_features=81, bias=True)
        (5): ReLU()
      )
      (decoder): Sequential(
        (0): Linear(in_features=81, out_features=500, bias=True)
        (1): ReLU()
        (2): Linear(in_features=500, out_features=500, bias=True)
        (3): ReLU()
        (4): Linear(in_features=500, out_features=784, bias=True)
        (5): Sigmoid()
      )
    )




```python
testset_all = np.array([testset_all[i][0].numpy() for i in range(len(testset_all))])
```


```python
true_error_all, losslist_all = train_test(best_model_all, best_model_optim_all, best_model_criterion_all, 100, device, trainloader_all, testset_all)
# best_model_all = denoising_model(**[results[str(j)]["params"] for j in range(27) if results[str(j)]["true_error"] == min([results[str(i)]["true_error"] for i in range(27)])][0])
# best_model_all.load_state_dict(torch.load("model_all", map_location=device))
# best_model_all.eval()
```

    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [08:15<00:00,  4.95s/it]
    


```python
true_error_all
```




    0.014089820474208327




```python
plt.plot(losslist_all)
```
    
![png](../assets/posts/autoencodeur/DenoisingAutoencoder/output_56_1.png)
    


On observe que la loss de notre mod√®le d√©croit au fil des epochs en se rapprochant de 0. 
Apr√®s 100 epochs, on peut voir qu'on peut potentiellement atteindre une loss plus basse en augmentant le nombre d'√©pochs. Cependant, c'est un mod√®le tr√®s lourd (toutes les classes sont comprises) et l'entra√Ænement peut prendre plusieurs 10aines de minutes.


```python
torch.save(best_model_all.state_dict(), "model_all")  # We save the model state dict at PATH   
```


```python
f,axes= plt.subplots(4,4,figsize=(5,5))
axes[0,0].set_title("Original Image")
axes[0,1].set_title("Dirty Image")
axes[0,2].set_title("Latent Space")
axes[0,3].set_title("Cleaned Image")
for i, noise in enumerate(["gaussian", "speckle", "salt_pepper", ""]):
    clean = xtest_all[50]
    dirty = add_noise(np.array(clean), noise, 0.8)
    predicted = best_model_all(torch.tensor(dirty.flatten()).type(torch.FloatTensor))

    axes[i, 0].imshow(clean)
    axes[i, 1].imshow(dirty.reshape((28, 28)))
    axes[i, 2].imshow(best_model_all.encode(torch.tensor(dirty.flatten()).type(torch.FloatTensor)).detach().numpy().reshape((9, 9)))
    axes[i, 3].imshow(predicted.detach().numpy().reshape((28, 28)))
```


    
![png](../assets/posts/autoencodeur/DenoisingAutoencoder/output_59_0.png)
    



```python
f,axes= plt.subplots(4,4,figsize=(5,5))
axes[0,0].set_title("Original Image")
axes[0,1].set_title("Dirty Image")
axes[0,2].set_title("Latent Space")
axes[0,3].set_title("Cleaned Image")
for i, noise in enumerate(["gaussian", "speckle", "salt_pepper", ""]):
    clean = xtest_all[51]
    dirty = add_noise(np.array(clean), noise, 0.8)
    predicted = best_model_all(torch.tensor(dirty.flatten()).type(torch.FloatTensor))

    axes[i, 0].imshow(clean)
    axes[i, 1].imshow(dirty.reshape((28, 28)))
    axes[i, 2].imshow(best_model_all.encode(torch.tensor(dirty.flatten()).type(torch.FloatTensor)).detach().numpy().reshape((9, 9)))
    axes[i, 3].imshow(predicted.detach().numpy().reshape((28, 28)))
```


    
![png](../assets/posts/autoencodeur/DenoisingAutoencoder/output_60_0.png)
    


Nous observons que le mod√®le poss√©dant toutes les classes est capable de reconstituer les images malgr√© un bruit important. 
Le pr√©c√©dent mod√®le n'ayant pas eu d'images autre que 1 et 7 n'√©tait pas capable de reconstituer des images d'autres chiffres. Ici, on observe bien le principe d'apprentissage de repr√©sentation via la zone latente, o√π multiples classes sont apprises par l'encodeur et ainsi reconstitu√©es par le d√©codeur.


```python
plt.imshow(best_model_all.encode(torch.tensor(dirty.flatten()).type(torch.FloatTensor)).detach().numpy().reshape((9, 9)))
```




    <matplotlib.image.AxesImage at 0x734b9fd3fd30>




    
![png](../assets/posts/autoencodeur/DenoisingAutoencoder/output_62_1.png)
    


Il est possible d'extraire la zone latente entre l'encodeur et le decodeur. Cette zone latente est la version la plus r√©duite de l'image (9x9 pixels contre 28x28 √† l'origine). C'est ce type de donn√©es qui va permettre au mod√®le de r√©aliser l'apprentissage de repr√©sentation. Ce type de donn√©es sera utilis√© plus tard dans l'√©tude pour faire un Encodeur-MLP : 

- un classifieur ayant comme architecture encodeur -> zone latente -> input pour MLP -> multi-classification.

---


# Impl√©mentation de nouvelles architectures d√©riv√©es <a class="anchor" id="impl√©mentation-de-nouvelles-architectures-d√©riv√©es"></a>


```python
best_model_all.load_state_dict(torch.load("model_all", map_location=device))
```




    <All keys matched successfully>



De mani√®re √©quivalente √† `train_test`, la fonction `train_classifier` permet d'entra√Æner un mod√®le en sp√©cifiant un grand nombre de param√®tres. Cependant, la partie test se fait en dehors pour des raisons de simplicit√©.

La fonction commence par mettre le mod√®le en mode d'entra√Ænement (`model.train()`). Ensuite, elle it√®re sur le nombre sp√©cifi√© d'√©poques, effectuant une passe avant et une passe arri√®re (r√©tropropagation) √† chaque it√©ration sur les donn√©es d'entra√Ænement.

Pour chaque lot de donn√©es dans `trainloader`, la fonction effectue les √©tapes suivantes :
1. Remise √† z√©ro des gradients (`optimizer.zero_grad()`).
2. Passage avant pour obtenir les sorties pr√©dites par le mod√®le.
3. Calcul de la perte entre les sorties pr√©dites et les vraies √©tiquettes √† l'aide de la fonction de perte sp√©cifi√©e (`criterion`).
4. R√©tropropagation de la perte pour calculer les gradients des param√®tres du mod√®le (`loss.backward()`).
5. Mise √† jour des poids du mod√®le en utilisant l'optimiseur (`optimizer.step()`).
6. Calcul de la perte cumul√©e pour cette √©poque.

√Ä la fin de chaque √©poque, la perte moyenne sur les donn√©es d'entra√Ænement est calcul√©e en divisant la somme des pertes cumul√©es par le nombre total de lots (`len(trainloader)`), puis ajout√©e √† la liste `losslist`.

Enfin, la fonction retourne la liste `losslist`, contenant les valeurs de la fonction de perte moyenne sur les donn√©es d'entra√Ænement pour chaque √©poque.


```python
def train_classifier(
        model: torch.nn.Module, 
        optimizer: torch.optim.Optimizer, 
        criterion: torch.nn.Module, 
        epochs: int,
        device: str,
        trainloader: torch.utils.data.DataLoader):
    """
    Fonction pour entra√Æner / tester un mod√®le.

    Parameters:
    -----------
    model : torch.nn.Module
        Le mod√®le √† entra√Æner.
    optimizer : torch.optim.Optimizer
        L'optimiseur utilis√© pour la mise √† jour des poids du mod√®le.
    criterion : torch.nn.Module
        La fonction de perte utilis√©e pour √©valuer la diff√©rence entre les pr√©dictions et les vraies √©tiquettes.
    epochs : int
        Le nombre d'√©poques d'entra√Ænement.
    device : str
        L'appareil sur lequel ex√©cuter l'entra√Ænement, par exemple 'cuda' pour GPU ou 'cpu' pour CPU.
    trainloader : torch.utils.data.DataLoader
        Le DataLoader contenant les donn√©es d'entra√Ænement.

    Returns:
    --------
    losslist : list
        Liste des valeurs de la fonction de perte moyenne sur les donn√©es d'entra√Ænement pour chaque √©poque.
    """
    model.train()
    losslist = []
    for epoch in tqdm(range(epochs)):
        running_loss = 0
        for output, label in trainloader:  
            optimizer.zero_grad()

            # Forward pass
            output = model(output)
            label = label.clone().detach()
            loss = criterion(output, label)

            # R√©tropropagation
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()

        # Calcul de la perte moyenne pour cette √©poque
        losslist.append(running_loss/len(trainloader))

    # Calcul de l'erreur moyenne quadratique sur le jeu de donn√©es de test

    return losslist
```

### D√©finition du classifier Benchmark CNN <a class="anchor" id="d√©finition-du-classifier-benchmark-cnn"></a>

**L'objectif est de comparer ce classifier benchmark a une architecture Encoder-MLP**.

Ce mod√®le CNN (Convolutional Neural Network) comprend deux couches de convolution suivies chacune d'une fonction d'activation ReLU et d'une op√©ration de max pooling. De plus, il y a une couche enti√®rement connect√©e √† la fin. Ce mod√®le est con√ßu pour traiter des images en niveaux de gris de taille 28x28 pixels.


```python
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 1, kernel_size=5, stride=1, padding=2)
        self.conv2 = nn.Conv2d(1, 1, kernel_size=2, stride=1)
        self.fc = nn.Linear(1 * 6 * 6, 10)

    def forward(self, x):
        x = x.view(-1, 1, 28, 28)
        x = torch.nn.functional.relu(self.conv1(x))
        x = torch.nn.functional.max_pool2d(x, kernel_size=2, stride=2)
        x = torch.nn.functional.relu(self.conv2(x))
        x = torch.nn.functional.max_pool2d(x, kernel_size=2, stride=2)
        x = x.view(-1, 1 * 6 * 6)
        x = self.fc(x)
        x = torch.nn.functional.softmax(x, dim=1)
        return x
```

Plus t√¥t dans l'√©tude, nous avons d√©crit notre fonction `train_classifier` mais selon la m√©thodologie, il est important de pouvoir tester notre mod√®le sur des donn√©es incoonues pour √©valuer sa g√©n√©ralisation.

La `true_error_func`, est con√ßue pour calculer le taux d'erreur sur un ensemble de donn√©es de test pour un mod√®le donn√©, ainsi nous donnant un indicateur de la qualit√© de sa g√©n√©ralisation.

- **Description** :
  1. L'assertion `assert len(xtest) == len(ytest), "Pas la meme taille entre le x et y"` v√©rifie que les donn√©es de test `xtest` et les √©tiquettes `ytest` ont la m√™me longueur. Si elles ne sont pas de la m√™me taille, un message d'erreur est affich√©.
  2. La variable `true_error` est initialis√©e √† 0 pour stocker le nombre d'erreurs de classification.
  3. La fonction parcourt les donn√©es de test et leurs √©tiquettes correspondantes √† l'aide d'une boucle `for i in range(len(xtest))`.
  4. Pour chaque exemple de test, la fonction compare la pr√©diction du mod√®le avec l'√©tiquette r√©elle :
     - Si la pr√©diction du mod√®le n'est pas √©gale √† l'index de l'√©tiquette r√©elle (obtenu en utilisant `torch.argmax`), cela signifie qu'une erreur de classification s'est produite. Dans ce cas, 1 est ajout√© √† la variable `true_error`.
     - Sinon, aucune erreur n'est ajout√©e √† `true_error`.
  5. Apr√®s avoir parcouru toutes les donn√©es de test, la fonction divise le nombre total d'erreurs (`true_error`) par le nombre total d'exemples de test pour obtenir le taux d'erreur moyen.
  6. Le taux d'erreur moyen est renvoy√© comme sortie de la fonction.


```python
def true_error_func(model: torch.nn.Module, 
                    xtest: torch.Tensor, 
                    ytest: torch.Tensor) -> float:
    """
    Calcule le taux d'erreur sur un ensemble de donn√©es de test pour un mod√®le donn√©.

    Parameters:
    -----------
    model : torch.nn.Module
        Le mod√®le de r√©seau de neurones √† √©valuer.
    xtest : torch.Tensor
        Les donn√©es de test en entr√©e.
    ytest : torch.Tensor
        Les √©tiquettes correspondantes des donn√©es de test.

    Returns:
    --------
    float
        Le taux d'erreur moyen sur l'ensemble de donn√©es de test.
    """
    assert len(xtest) == len(ytest), "Pas la meme taille entre le x et y"
    true_error = 0
    for i in range(len(xtest)):
        true_error += 1 if ytest[i] != torch.argmax(model(xtest[i])) else 0
    true_error /= len(xtest)
    return true_error

```


```python
cnn_benchmark = CNN()
bench_optim = optim.Adam(cnn_benchmark.parameters(), 1e-4)
bench_crite = nn.CrossEntropyLoss()
batch_size=128
bench_data = []

for img in tqdm(traindata_all):
    bench_data.append(torch.tensor(img).type(torch.float32))
bench_training_data = DataLoader(list(zip(bench_data, ytrain_all)), batch_size=batch_size, shuffle=False)
```

    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60000/60000 [00:00<00:00, 79623.46it/s]
    


```python
losslist_classif_benchmark = train_classifier(cnn_benchmark, bench_optim, bench_crite, 150, device, bench_training_data)
```

    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [06:53<00:00,  2.76s/it]
    


```python
plt.plot(losslist_classif_benchmark)
```
   
![png](../assets/posts/autoencodeur/DenoisingAutoencoder/output_75_1.png)
    



```python
precision = 0
for i in range(len(xtest_all)):
    precision += 1 if ytest_all[i] == torch.argmax(cnn_benchmark(torch.tensor(xtest_all[i]).type(torch.float32))) else 0
precision /= len(xtest_all)
precision
```




    0.8706



#### R√©sultat Benchmark CNN <a class="anchor" id="r√©sultat-benchmark-cnn"></a>

Les r√©sultats pr√©c√©dents nous montrent que notre mod√®le CNN n'est pas de tr√®s grande qualit√©. Pendant son entra√Ænement, la loss n'atteint pas de valeur en dessous de 1, ce qui laisse √† d√©sirer la qualit√© de la classification. Puis, en calculant sont accuracy sur le jeu de test, on trouve 87% et 12% d'erreur. Ce mod√®le cr√©√© une bonne baseline sur laquelle comparer le mod√®le suivant.

### Definition du classifier (Encoder-MLP) <a class="anchor" id="definition-du-classifier-encoder-mlp"></a>

**Ce mod√®le entre en comp√©tition avec notre claassifier CNN**


Ce classifieur va prendre en entr√©e la zone latente (sortie de l'encodeur) et va nous servir de baseline en termes d'architecture Encodeur-Classifier.

Il s'agit d'un classificateur de type MLP :  `small_classifier`.

- **Initialisation** : La m√©thode `__init__` initialise les param√®tres du r√©seau. Elle prend en argument `hidden_size` (la taille des couches cach√©es), `hidden_nb` (le nombre de couches cach√©es), `input_size` (la taille de l'entr√©e) et `output_size` (la taille de la sortie). √Ä l'int√©rieur de la m√©thode `__init__` :
  - Une liste appel√©e `sequence_input` est cr√©√©e, contenant une couche lin√©aire (`nn.Linear`) suivie d'une fonction d'activation ReLU (`nn.ReLU()`). Cela repr√©sente la couche d'entr√©e.
  - Une autre liste appel√©e `sequence_output` est cr√©√©e, contenant une couche lin√©aire suivie d'une fonction d'activation softmax (`nn.Softmax(dim=1)`), repr√©sentant la couche de sortie.
  - Une liste vide appel√©e `sequence_hidden` est cr√©√©e pour contenir les couches cach√©es.
  - Une boucle it√®re `hidden_nb` fois, ajoutant des paires de couches lin√©aires suivies de fonctions d'activation ReLU √† `sequence_hidden`.
  - Toutes ces s√©quences de couches sont combin√©es dans une liste appel√©e `classif_sequence`, repr√©sentant la s√©quence de couches dans le classificateur.

- **Mod√®le s√©quentiel** : Les couches d√©finies dans `classif_sequence` sont ensuite pass√©es √† `nn.Sequential()`, qui cr√©e un mod√®le de r√©seau de neurones compos√© de ces couches en s√©quence. Ce mod√®le s√©quentiel est stock√© dans l'attribut `self.classifier`.

- **Propagation avant** : La m√©thode `forward` d√©finit comment les donn√©es d'entr√©e `x` circulent √† travers le r√©seau lors de la propagation avant. Elle passe simplement l'entr√©e `x` √† travers le mod√®le s√©quentiel (`self.classifier`) et renvoie la sortie.

En r√©sum√©, cette classe d√©finit un classificateur de r√©seau de neurones simple avec un nombre personnalisable de couches cach√©es, chacune avec une taille sp√©cifi√©e et une fonction d'activation ReLU. Les couches d'entr√©e et de sortie sont √©galement personnalisables, la couche de sortie utilisant une activation softmax pour les t√¢ches de classification multiclasses.


```python
class Encoder_MLP_classifier(nn.Module):
    def __init__(self, hidden_size, hidden_nb, input_size=81, output_size=10) -> None:
        super().__init__()

        sequence_input = [nn.Linear(input_size, hidden_size), nn.ReLU()]
        sequence_output = [nn.Linear(hidden_size, output_size), nn.Softmax(dim=1)]
        sequence_hidden = []
        for i in range(hidden_nb):
            sequence_hidden.append(nn.Linear(hidden_size, hidden_size))
            sequence_hidden.append(nn.ReLU())

        classif_sequence = sequence_input + sequence_hidden + sequence_output
            
        self.classifier = nn.Sequential(
            *classif_sequence
        )
    
    def forward(self, x):
        return self.classifier(x)
```

### Cr√©ation du jeu d'entra√Ænement avec les zones latentes <a class="anchor" id="cr√©ation-du-jeu-dentra√Ænement-avec-les-zones-latentes"></a>

La case suivante initialise le jeu `training_data` constitu√© de toutes les zones latentes apr√®s le passage des images du jeu `traindata_all` dans l'encodeur que nous avons impl√©ment√© plus haut. Cette mani√®re d'initialiser le jeu de donn√©es nous √©vite beaucoup de difficult√©s sur la r√©solution des dimensions si cette op√©ration √©tait faite dans la boucle d'entra√Ænement. De cette mani√®re, nous avons un dataloader qui peut √™tre parcouru par notre mod√®le `small_classifier`de type MLP.


```python
encoder_output = []
for img in tqdm(traindata_all):
    encoder_output.append(best_model_all.encode(torch.tensor(img.flatten()).type(torch.float32)).clone().detach())
training_data = DataLoader(list(zip(encoder_output, ytrain_all)), batch_size=batch_size, shuffle=False)
```

    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60000/60000 [00:07<00:00, 7617.43it/s]
    


```python
# # on enregistre les r√©sultats pour ne pas devoir refaire la gridsearch
# with open("output_gridsearch_classif.json", "w+") as file:
#     file.write(json.dumps(results, indent=1))

# on lit les r√©sultats de l'ancienne gridsearch
with open("output_gridsearch_classif.json") as file:
    results = json.loads(file.read())
```


```python
classifier_encoder_mlp = Encoder_MLP_classifier(**[results[str(j)]["params"] for j in range(21) if results[str(j)]["true_error"] == min([results[str(i)]["true_error"] for i in range(21)])][0])
classif_optim = optim.Adam(classifier_encoder_mlp.parameters(), 5e-5)
classif_crite = nn.CrossEntropyLoss()
batch_size=16
print(classifier_encoder_mlp)
```

    Encoder_MLP_classifier(
      (classifier): Sequential(
        (0): Linear(in_features=81, out_features=20, bias=True)
        (1): ReLU()
        (2): Linear(in_features=20, out_features=20, bias=True)
        (3): ReLU()
        (4): Linear(in_features=20, out_features=20, bias=True)
        (5): ReLU()
        (6): Linear(in_features=20, out_features=20, bias=True)
        (7): ReLU()
        (8): Linear(in_features=20, out_features=20, bias=True)
        (9): ReLU()
        (10): Linear(in_features=20, out_features=20, bias=True)
        (11): ReLU()
        (12): Linear(in_features=20, out_features=10, bias=True)
        (13): Softmax(dim=1)
      )
    )
    


```python
losslist_classif = train_classifier(classifier_encoder_mlp, classif_optim, classif_crite, 120, device, training_data)
```

    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 120/120 [11:39<00:00,  5.83s/it]
    


```python
plt.plot(losslist_classif)
```




    [<matplotlib.lines.Line2D at 0x734b5b6253f0>]




    
![png](../assets/posts/autoencodeur/DenoisingAutoencoder/output_85_1.png)
    



```python
encode_xtest_all = []
for img in tqdm(xtest_all):
    encode_xtest_all.append(best_model_all.encode(torch.tensor(img.flatten()).type(torch.float32)).view(-1, 9, 9).clone().detach())
```

    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:01<00:00, 5737.65it/s]
    


```python
true_error_func(classifier_encoder_mlp, [encode_xtest_all[i].view(1, -1).type(torch.float32) for i in range(len(encode_xtest_all))], ytest_all)
```




    0.0872



### R√©sultats du classifieur Encoder-MLP <a class="anchor" id="r√©sultats-du-classifieur-encoder-mlp"></a>

Le calcul de l'erreur r√©elle avec la fonction `true_error_func` nous retourne une erreure r√©elle de 8% et accuracy 92%, ce qui est un meilleur score que le mod√®le CNN. Toutefois, la loss au cours de l'entra√Ænement ne descends pas en dessous de 1, ce qui remet en jeu la qualit√© de la classification. L'erreur r√©ell√© indique que notre mod√®le est capable de se g√©n√©raliser a des donn√©es inconnues. Mais nous ne pouvons pas assurer sa fiabilit√© avec la loss de son entra√Ænement. De mani√®re g√©n√©rale, le mod√®le Encodeur-MLP est meilleur que le mod√®le CNN.

--- 


### Gridsearch mod√®le Encodeur-MLP <a class="anchor" id="gridsearch-mod√®le-encodeur-mlp"></a>


Voici la gridsearch utilis√©e pour optimiser les param√®tres du mod√®le Encoder_MLP_classifier.


```python
encoder_output = []
for img in tqdm(traindata_all):
    encoder_output.append(best_model_all.encode(torch.tensor(img.flatten()).type(torch.float32)).clone().detach())
training_data = DataLoader(list(zip(encoder_output, torch.tensor(ytrain_all.reshape((-1, 1))).type(torch.float32))), batch_size=batch_size, shuffle=False)
```

    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60000/60000 [00:09<00:00, 6043.92it/s]
    


```python
def gridsearch_classif(model_obj: nn.Module, 
               parameters: dict,
               optimizer: optim.Optimizer,
               criterion,
               learning_rate :int,
               epoch: int,
               device: str,
               trainloader: DataLoader,
               xtestdata: Dataset,
               ytestdata):
    parameters = ParameterGrid(parameters)
    results = {}
    loss = criterion()

    for i, params in enumerate(parameters):
        tmp_model = model_obj(**params)
        tmp_optim = optimizer(tmp_model.parameters(), lr=learning_rate)
        tmp_result = train_classifier(tmp_model, tmp_optim, loss, epoch, device, trainloader)
        error_result = true_error_func(tmp_model, xtestdata, ytestdata)
        print(f"\tParams N¬∞{i+1}/{len(parameters)} -> true error: {error_result} / {params=}")
        results[i] = {
            "params": params,
            "true_error": error_result,
            "training_error": tmp_result
        }

    return results

parameters = {
    "hidden_nb": range(1, 6, 2),
    "hidden_size": range(20, 81, 10),
}

results = gridsearch_classif(Encoder_MLP_classifier, parameters, optim.Adam, nn.MSELoss, 1e-4, 25, device, training_data, [encode_xtest_all[i].view(1, -1).type(torch.float32) for i in range(len(encode_xtest_all))], ytest_all)
```


```python
[results[j]["params"] for j in range(21) if results[j]["true_error"] == min([results[i]["true_error"] for i in range(21)])][0]
```




    {'hidden_nb': 5, 'hidden_size': 20}




```python
with open("output_gridsearch_classif.json", "w+") as file:
    file.write(json.dumps(results, indent=1))
```

## Discussion sur les performances de mod√®les Encodeur-CNN et Encodeur-MLP <a class="anchor" id="discussion-sur-les-performances-de-mod√®les-encodeur-cnn-et-encodeur-mlp"></a>

Apr√®s avoir test√© notre Autoencodeur de d√©bruitage, nous avons d√©velopp√© des architectures d√©riv√©es qui utilisent l'apprentissage de repr√©sentation de notre encodeur (nottament la zone latente) afin d'√©valuer comment cette zone latente pouvait √™tre utilis√©e √† des fins de classification multiclasse.

La premi√®re architecture que nous avons consid√©r√© est la suivante :

```
 ____________________________________________________________________________________________
|                   Classification CNN model (simplified version)                            |
|____________________________________________________________________________________________|                 
|                                                                                            |
|                   CNN (Input: image_noisy)                                                 |
|   Input (latent zone) --> Conv2D Hidden Layers (MaxPooling2D) ... --> Output (Softmax)     |
|                                                                                            |
|____________________________________________________________________________________________|
```

Le but est que le CNN puisse √©tablir une baseline en terme de classification multiclasse. De m√™me que pour l'autoencoder, le CNN utilise des informations tr√®s basse dimensions similaires √† la zone latente. Il repr√©sente alors un bon concurrent pour le mod√®le Encodeur-MLP.
Les r√©sultats montrent que cette architecture atteint 87% d'accuracy avec 12% d'erreur. Ces performances diff√©rents des standards de la litt√©rature sur les donn√©es MNIST, mais il faut prendre en compte que les donn√©es sont bruit√©es, avec 3 variation de bruit diff√©rentes. Ceci enl√®ve les informations contextuelle de l'image qui ne permet pas au CNN d'extraire des informations n√©cessaire √† la reconnaissance du chiffre. En observant des images issues du jeu d'entra√Ænement, on voit bien que l'image manque de patterns et d'informations. En effet, les CNN sont con√ßus pour apprendre des repr√©sentations hi√©rarchiques des caract√©ristiques spatiales, en commen√ßant par des bords et des textures simples jusqu'√† des formes complexes et des parties d'objets. Si les donn√©es ne pr√©sentent pas cette structure hi√©rarchique, les couches suppl√©mentaires dans les CNN pourraient ne pas apporter d'avantages significatifs. Une architecture trop complexe comme les CNN serait alors inneficace pour ce genre de classification sur des images dont les informations contextuelles sont bruit√©es.


La seconde architecture que nous avons consid√©r√© est la suivante :

```
 ____________________________________________________________________________________________
|                   Classification Encoder-MLP model (simplified version)                    |
|____________________________________________________________________________________________|
|                                                                                            |
|                   Encoder (Input: img_size)                                                |
|   Input --> Hidden Layers (ReLU) ... --> Output (latent zone)                              |
|                                                                                            |
|                   MLP (Input: latent_size)                                                 |
|   Input (latent zone) --> Linear Layers (ReLU) ... --> Output (Softmax)                    |
|                                                                                            |
|____________________________________________________________________________________________|
```

Le but est que le MLP puisse utiliser la zone latente pour extraire des informations de cette derni√®re et effectuer une classification. Contrairement √† l'√©tude pr√©c√©dente, les r√©sultats sont tr√®s bons (8% d'erreur, 92% d'accuracy). Les MLP sont des r√©seaux enti√®rement connect√©s et peuvent apprendre des relations non lin√©aires complexes entre les caract√©ristiques d'entr√©e. Ils ne sont pas contraints par les relations spatiales dans les donn√©es, contrairement aux CNN qui supposent des corr√©lations spatiales locales dans l'entr√©e. Dans un espace de petite dimension comme la zone latente, les caract√©ristiques pertinentes pour la classification ne sont pas facilement discernables au niveau des pixels mais √©mergent plut√¥t d'une combinaison de valeurs de pixels, les MLP, grace √† leur couches cach√©es peuvent avoir une meilleure compr√©hension de l'information, sans avoir a chercher des informations contextuelles. Dans ce cas, l'architecture simpliste des MLP correspond √† la nature tr√®s simple des donn√©es de la zone latente.


**Nous pouvons conclure l'√©tude en affirmant que le mod√®le Encodeur-MLP est plus performant que le mod√®le standard CNN pour classifier des donn√©es bruit√©es.**





```python
torch.save(classifier_encoder_mlp.state_dict(), "classifieur")  # We save the model state dict at PATH
torch.save(cnn_benchmark.state_dict(), "benchmark")   
```



## Contributing ü§ù

Contributions are highly encouraged! If you have suggestions, improvements, or feature requests, feel free to reach out to me !

## License üìù

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

Developed by Pierre LAGUE and Fran√ßois MULLER at the University of Lille, France. üöÄüìä
